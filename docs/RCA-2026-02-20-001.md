# Root Cause Analysis — INC-2026-02-20-001

## Incident: Market Data Bridge API Unreachable via Cloudflare Tunnel

| Field | Value |
|-------|-------|
| **Document ID** | RCA-2026-02-20-001 |
| **Date of Incident** | 2026-02-20 |
| **Detected By** | ChatGPT CustomGPT (Agent #4) |
| **Detected At** | ~2026-02-20 15:30 UTC (10:30 AM ET) |
| **Resolved At** | 2026-02-20 15:41 UTC (10:41 AM ET) |
| **Duration** | **41 hours 14 minutes** (Feb 18 14:16 → Feb 20 07:40 silent; total with crash loop: ~48 hours) |
| **Severity** | Medium — external API integrations offline, no financial impact |
| **Affected Systems** | REST API (port 3000), Cloudflare Tunnel, ChatGPT CustomGPT |
| **Prepared By** | Claude Code (Agent #2, Staff Engineer) |
| **Reviewed By** | Pending — Human (Engineering Manager) |

---

## 1. Executive Summary

The Market Data Bridge REST API became unreachable through the Cloudflare Tunnel, causing the ChatGPT CustomGPT integration to receive Cloudflare error 1033 (Argo Tunnel error / no running origin). The root cause was that PM2 process manager had no running processes — the bridge server was not running, so the Cloudflare Tunnel (which was operational) had no origin server to proxy to.

---

## 2. Impact Assessment

| Impact Area | Assessment |
|-------------|-----------|
| **Trading operations** | None — no open positions, no active orders during outage |
| **Data loss** | None — SQLite DB (`data/bridge.db`) intact, no writes during downtime |
| **Financial** | None — assist-discretion mode, no automated execution |
| **External integrations** | ChatGPT CustomGPT unable to call bridge API |
| **MCP tools (Claude Code)** | Unaffected — MCP connects to TWS directly, not through REST |
| **Monitoring** | None — no alerting system detected the outage |

---

## 3. Timeline of Events

| Timestamp (ET) | Event | Source |
|----------------|-------|--------|
| 2026-02-17 19:30:41 | Last successful health check logged (`GET /health/deep → 200`) | `pm2-error.log:1883` |
| 2026-02-17 ~23:30 | PM2 pidusage crashes — system already struggling | `pm2.log`: `pidusage wmic exited code 1073807364` |
| 2026-02-18 04:18 | **1st unexpected shutdown** — system crashed or lost power | Event ID 41 + 6008 |
| 2026-02-18 05:11 | PM2 daemon restarted, loaded bridge + paper from dump file | `pm2.log:81` — daemon start #2 |
| 2026-02-18 06:49–10:11 | **Reboot storm: 7 more unexpected shutdowns + 6 user-initiated restarts** in 5 hours | Event IDs 41, 6008, 1074 (×6, via `StartMenuExperienceHost.exe` on behalf of `dotca`) |
| 2026-02-18 07:40 | PM2 daemon restarted (start #3), loaded bridge only | `pm2.log:123` |
| 2026-02-18 07:58 | **`PM2 successfully stopped`** — deliberate `pm2 kill` command | `pm2.log:151` — **ACTOR: user `dotca` (during reboot storm troubleshooting)** |
| 2026-02-18 10:14 | System finally stable (uptime sustained) | Event ID 6005 + 6013 (uptime 31s → sustained) |
| 2026-02-18 14:16 | PM2 daemon restarted (start #4) — **empty process list** (dump file corrupted/stale from reboot storm) | `pm2.log:153` — daemon alive, no apps |
| 2026-02-18 14:16 → 2026-02-20 07:40 | **PM2 daemon running with ZERO apps for 41 hours** — no reboots in this window | System Event Log confirms zero shutdown events |
| 2026-02-20 07:40 | PM2 loaded apps from dump — bridge enters **crash loop** (28 restarts in 10 min, SIGINT after ~7s each) | `pm2.log:169+` — EADDRINUSE pattern |
| 2026-02-20 ~08:00 | Crash loop exhausts max_restarts, bridge stops retrying | PM2 exponential backoff → 15s intervals |
| 2026-02-20 15:39 | Cloudflare tunnel listed as running (PID 58460), 0 connections | `cloudflared tunnel list` |
| 2026-02-20 15:39 | PM2 process list confirmed empty (`pm2 list` — no processes) | Terminal |
| 2026-02-20 15:40:20 | Bridge restarted via `pm2 start ecosystem.config.cjs` | `pm2-error.log:15439` |
| 2026-02-20 15:40:25 | **EADDRINUSE on :3000** — first restart attempt failed (port conflict from stale process) | `pm2-error.log:15490` |
| 2026-02-20 15:40:29 | Second crash loop — EADDRINUSE again | `pm2-error.log:15582` |
| 2026-02-20 15:40:34 | Third crash loop — EADDRINUSE again (clientId=8) | `pm2-error.log:15611` |
| 2026-02-20 15:41:29 | Bridge stabilized on 3rd restart attempt (port freed, PID 45660) | `pm2-error.log` (scheduler armed) |
| 2026-02-20 15:41:47 | IBKR connected (clientId=11, live, port=7496) | `pm2-error.log` |
| 2026-02-20 15:41:47 | Boot reconciliation completed (0 orders) | `pm2-error.log` |
| 2026-02-20 15:43:39 | `GET /api/status → 200` confirmed via localhost | Terminal |

---

## 4. Root Cause Analysis

### 4.1 Direct Cause

PM2 process list was empty. No bridge server was running on port 3000. The Cloudflare Tunnel (cloudflared, PID 58460, running as a Windows service) was operational but had no origin to proxy to, resulting in Cloudflare error 1033.

### 4.2 Root Cause — Windows Update Reboot Storm + Manual `pm2 kill`

**The actor has been identified.** Forensic evidence from Windows Event Viewer and the PM2 daemon log (`~/.pm2/pm2.log`) reveals the following causal chain:

1. **Feb 18 04:18 ET — Windows crashed** (Event ID 41: "rebooted without cleanly shutting down first"). This was the first of **8 unexpected shutdowns** and **6 user-initiated restarts** (Event ID 1074, initiated by `StartMenuExperienceHost.exe` on behalf of user `dotca`) over 6 hours. The Windows Update Client log confirms Defender Intelligence Updates (KB2267602) were installing during this period.

2. **PM2 daemon restarted 3 times** across these reboots (starts at 05:11, 07:40, and 14:16 ET), each time attempting to restore apps from the dump file (`~/.pm2/dump.pm2`).

3. **Feb 18 07:58 ET — User ran `pm2 kill`** (`pm2.log:151`: `PM2 successfully stopped`). This was a deliberate command, likely issued by user `dotca` while troubleshooting the reboot storm. This killed the PM2 daemon and all managed processes.

4. **Feb 18 14:16 ET — PM2 daemon restarted** (start #4), but this time **no apps were loaded**. The dump file was either corrupted by the reboot storm or emptied by the `pm2 kill`. The daemon sat running with an empty process list.

5. **Feb 18 14:16 → Feb 20 10:30 ET — 41+ hours of silent outage**. The PM2 daemon was alive but managing zero apps. Zero reboots occurred in this window (confirmed by absence of Event IDs 41/1074/6005/6006/6008 in System log). No monitoring detected the condition.

6. **Feb 20 07:40 ET — PM2 loaded apps from dump** (possibly triggered by a `pm2 start` command or dump restoration). The bridge entered a crash loop: 28 restarts in 10 minutes, each crashing with SIGINT after ~7 seconds (EADDRINUSE pattern from stale process on port 3000).

**Confirmed actor:** User `dotca` (via `pm2 kill` at 07:58 ET during reboot storm troubleshooting).

**Contributing factor:** The dump file (`dump.pm2`) did not reliably persist the app configuration across the reboot storm + explicit kill sequence. When PM2 daemon #4 started at 14:16, it had no apps to restore.

**Note:** The dump file currently contains environment variables from a Claude Desktop session (`CLAUDE_CODE_ENTRYPOINT: "claude-desktop"`), confirming the bridge was most recently started from an AI agent session, not a manual terminal.

### 4.3 Contributing Cause — EADDRINUSE on Restart

When `pm2 start` was run (both at 07:40 on Feb 20 and at 15:40 on Feb 20), the bridge crashed with `EADDRINUSE: address already in use :::3000`. This indicates a stale node.exe process was still holding port 3000 from a previous invocation.

On Feb 20 07:40, this caused a **28-iteration crash loop** (exponential backoff from 1s to max 15s) before the process apparently stabilized or gave up. At 15:40, PM2's restart strategy succeeded on the 3rd attempt after the stale process released the port.

### 4.4 Contributing Cause — Asymmetric Service Architecture

Cloudflared IS configured as a Windows service (runs in Session 0, survives reboots). PM2 is NOT — it runs as a user-space daemon under `dotca`'s login session. This asymmetry is the architectural root cause: any event that disrupts the user session (reboot, logoff, `pm2 kill`) kills the bridge while the tunnel keeps running, creating the error 1033 condition.

### 4.4 Why It Wasn't Detected Sooner

| Gap | Detail |
|-----|--------|
| No uptime monitoring | No external ping/heartbeat checks the tunnel URL |
| No PM2 persistence | PM2 doesn't auto-start on boot (Windows) |
| No alerting | No Slack/email/webhook fires when bridge goes down |
| Tunnel health monitor is internal | The bridge's own tunnel monitor (`scheduler.ts`) can't run if the bridge itself is down |
| ChatGPT was the only external consumer | It detected the issue when it tried to call the API |

---

## 5. Five Whys

1. **Why was the API unreachable?** — Cloudflare Tunnel had no origin server to proxy to.
2. **Why was there no origin server?** — PM2 had zero apps in its process list for 41+ hours.
3. **Why was PM2 empty?** — User ran `pm2 kill` at 07:58 ET on Feb 18 during a Windows Update reboot storm (8 crashes in 6 hours). When the PM2 daemon restarted at 14:16, the dump file was empty/corrupted, so no apps were restored.
4. **Why didn't PM2 recover automatically?** — PM2 is not registered as a Windows service. Its `pm2 save` + `pm2-startup` persistence was never configured. The dump file (`dump.pm2`) is fragile under rapid crash/kill cycles.
5. **Why wasn't the 41-hour outage detected?** — No external monitoring exists. The bridge's own health checks (internal) can't fire when the bridge is down. ChatGPT CustomGPT was the accidental canary.

---

## 6. Corrective Actions

### Immediate (completed)

| # | Action | Status | Owner |
|---|--------|--------|-------|
| CA-1 | Restarted bridge via `pm2 start ecosystem.config.cjs` | ✅ Done | Claude Code |
| CA-2 | Verified API responding (`/api/status → 200`) | ✅ Done | Claude Code |
| CA-3 | Verified IBKR connected (clientId=11, live, port=7496) | ✅ Done | Claude Code |

### Short-term (this week)

| # | Action | Status | Owner | Target |
|---|--------|--------|-------|--------|
| CA-4 | Configure PM2 startup persistence on Windows (`pm2 save` + startup service) | ⏳ Open | Amazon Q (#14) / Copilot (#5) | Feb 24 |
| CA-5 | Add external uptime ping (e.g., UptimeRobot free tier → tunnel URL `/health/ready`) | ⏳ Open | Human | Feb 24 |
| CA-6 | Document boot procedure in ops runbook | ⏳ Open | Copilot (#5) | Feb 24 |

### Long-term (this quarter)

| # | Action | Status | Owner | Target |
|---|--------|--------|-------|--------|
| CA-7 | Implement dead-man's-switch: if bridge hasn't reported to external monitor in 10min, send alert | ⏳ Open | Claude Code (#2) | Q1 2026 |
| CA-8 | Add PM2 process count to `ops_health` MCP tool output | ⏳ Open | Claude Code (#2) | Q1 2026 |
| CA-9 | Consider migrating PM2 to a Windows Service (nssm or node-windows) for OS-level reliability | ⏳ Open | Amazon Q (#14) | Q2 2026 |

---

## 7. Lessons Learned

| # | Lesson | Category |
|---|--------|----------|
| LL-1 | Cloudflared running as a service + PM2 NOT running as a service = asymmetric reliability. Any component that must survive reboots needs OS-level service registration. | Architecture |
| LL-2 | Internal health monitors (bridge checking its own tunnel) create a circular dependency — the monitor can't detect that IT is down. External monitoring is required. | Monitoring |
| LL-3 | The ChatGPT CustomGPT integration was the first external consumer to hit this — it effectively served as an accidental canary. Production integrations need dedicated monitoring, not accidental detection. | Process |
| LL-4 | EADDRINUSE on restart indicates stale processes from manual `npm start` invocations. Enforce: all production runs go through PM2, never bare `npm start`. | Operations |
| LL-5 | The 2-day gap between Feb 18 and Feb 20 with no logs and no alerts means this could have been a 2-day outage without anyone noticing. Unacceptable for a system with external API consumers. | Risk |

---

## 8. Risk Re-Assessment

| Risk | Before Incident | After Incident | After CAs Complete |
|------|----------------|----------------|-------------------|
| Bridge down, undetected | **High** (no monitoring) | **High** (still no monitoring) | **Low** (external ping + alerts) |
| PM2 lost on reboot | **High** (no persistence) | **High** (still no persistence) | **Low** (service registration) |
| Stale port conflict on restart | **Medium** (manual starts) | **Medium** | **Low** (PM2-only policy) |
| External API consumer impacted | **Medium** (1 consumer) | **Medium** | **Low** (monitoring + alerts) |

---

## 9. Document Control

| Version | Date | Author | Change |
|---------|------|--------|--------|
| 1.0 | 2026-02-20 | Claude Code (#2) | Initial RCA |
| 1.1 | 2026-02-20 | Claude Code (#2) | **True root cause identified**: Windows Update reboot storm (8 crashes) + explicit `pm2 kill` by user `dotca` at 07:58 ET. Timeline updated with Windows Event Log + PM2 daemon log forensics. Five Whys revised. |

**Next review date:** 2026-02-24 (after CA-4, CA-5, CA-6 implemented)

**Approval:**
- [ ] Engineering Manager (dotcal604)
- [ ] Corrective actions assigned to owners
- [ ] Follow-up review scheduled
